{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac408b5e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc639f114a2869c4d2f09a3fb9299a08",
     "grade": false,
     "grade_id": "cell-b00828259c8e42e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# RO47019: Intelligent Control Systems Practical Assignment\n",
    "* Period: 2024-2025, Q4\n",
    "* Course homepage: https://brightspace.tudelft.nl/d2l/home/682445\n",
    "* Instructor: Cosimo Della Santina (C.DellaSantina@tudelft.nl)\n",
    "* Teaching assistant: Niels Stienen (N.L.Stienen@student.tudelft.nl)\n",
    "* (c) TU Delft, 2025\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or `YOUR ANSWER HERE` and remove `raise NotImplementedError()` afterwards. Moreover, if you see an empty cell, please **do not** delete it, instead run that cell as you would run all other cells. Finally, please **do not** add any extra cells to this notebook or change the existing cells unless you are explicitly asked to do so.\n",
    "\n",
    "Please fill in your name(s) and other required details below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f9e220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please fill in your names, student numbers, netID, and emails below.\n",
    "STUDENT_1_NAME = \"Thomas prins\"\n",
    "STUDENT_1_STUDENT_NUMBER = \"5885221\"\n",
    "STUDENT_1_NETID = \"tsprins\"\n",
    "STUDENT_1_EMAIL = \"T.S.Prins@student.tudelft.nl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ba32571",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "042927213b84aa368aa3ea72caa4cb60",
     "grade": true,
     "grade_id": "cell-9f148ec62e0de49c",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Note: this block is a check that you have filled in the above information.\n",
    "# It will throw an AssertionError until all fields are filled\n",
    "assert STUDENT_1_NAME != \"\"\n",
    "assert STUDENT_1_STUDENT_NUMBER != \"\"\n",
    "assert STUDENT_1_NETID != \"\"\n",
    "assert STUDENT_1_EMAIL != \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af317a94",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1ac82f761cd49715da5f2adb9bea9f2",
     "grade": false,
     "grade_id": "cell-4ea391677951116c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### General announcements\n",
    "\n",
    "* Do *not* share your solutions (also after the course is finished), and do *not* copy solutions from others. By submitting your solutions, you claim that you alone are responsible for this code.\n",
    "\n",
    "* Please post your questions regarding this assignment in the correct support forum on Brightspace, this way everybody can benefit from the response. Please note that it is **not** allowed to post any code relating to solution attempts. If you do have a particular question that you want to ask directly, please use the scheduled Q&A hours to ask the TA or if not possible otherwise, send an email to the instructor or TA.\n",
    "\n",
    "* This notebook will have in various places a line that throws a `NotImplementedError` exception. These are locations where the assignment requires you to adapt the code! These lines are just there as a reminder for you that you have not yet adapted that particular piece of code, especially when you execute all the cells. Once your solution code replaced these lines, it should accordingly *not* throw any exceptions anymore.\n",
    "\n",
    "* This [Jupyter notebook](https://jupyter.org/) uses `nbgrader` to help us with automated tests. `nbgrader` will make various cells in this notebook \"uneditable\" or \"unremovable\" and gives them a special id in the cell metadata. This way, when we run our checks, the system will check the existence of the cell ids and verify the number of points and which checks must be run. While there are ways that you can edit the metadata and work around the restrictions to delete or modify these special cells, you should not do that since then our nbgrader backend will not be able to parse your notebook and give you points for the assignment. \n",
    "\n",
    "* Please note that the above mentioned _read-only_ protection only works in Jupyter Notebook, and it does not work if you open this notebook in another editor (e.g., VSCode, PyCharm, etc.). Therefore, we recommend that you only use Jupyter Notebook for this course. If you use any other editor, you may accidentally delete cells, modify the tests, etc., which would cause you to lose points.\n",
    "\n",
    "* If you edit a function that is imported in another notebook, you need to **restart the kernel** of the notebook where you are using the function. Otherwise, the changes will not be effective.\n",
    "\n",
    "* **IMPORTANT**: Please make sure that your code executes without any errors before submitting the notebook. An easy way to ensure this is to use the validation script as described in the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c956945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32ecec16-2871-4af9-9c97-559a00f4eecc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2022becab2dd27b5dc44201ec01ab827",
     "grade": false,
     "grade_id": "cell-f5e628962eac03a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Task 1b - Vision-based angle prediction (20 points)\n",
    "\n",
    "**Authors:** Chuhan Zhang (C.Zhang-8@tudelft.nl), Tomás Coleman, Maximilian Stölzle\n",
    "\n",
    "**Warning** To help you, we have marked parts of the code where we want you to contribute. These parts are often marked with a start comment, `Task 1b.i.j: ...` and a closing comment, `Task 1b.i.j: END`. You are free to add code outside of the designated areas. However, we cannot guarantee that this won't affect the intended behavior of the code.\n",
    "\n",
    "In this file, we will implement two Convolutional Neural Network (CNN) models to predict the position of the single pendulum based on input image data. Each CNN model will try a separate way to predict angles: direct or indirect. Direct prediction means that the network outputs a prediction of the link angle $\\hat \\theta$, and then updates the network parameters by comparing the loss between the predicted $\\hat \\theta$ and the label $\\theta$. However, indirect prediction means that the dataset has changed. Label is no longer a single angle, but an array containing the $\\sin \\theta$ value and $\\cos \\theta$ value of the angle. The output of the network is no longer a single $\\hat \\theta$, but the predicted $[\\hat {\\sin \\theta}, \\hat {\\cos \\theta}]$. The network updates parameters by comparing the loss between $[\\sin \\theta, \\cos \\theta]$ and $[\\hat {\\sin \\theta}, \\hat {\\cos \\theta}]$.\n",
    "\n",
    "The following cells import all the necessary packages and external functions to run the code properly. First, we want to check whether the images are generated properly and create the directory for output plots and models. Different dataset classes are also created from the information gathered in notebook 1a. Finally, different Pytorch data loaders will be created for each network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "badaa419-10cc-4700-8bcb-643be394f636",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "128221cdc7cb27bafa9f1571447b2313",
     "grade": false,
     "grade_id": "cell-95396aef1ca5b8b3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os import PathLike\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split\n",
    "from tqdm.notebook import tqdm  # progress bar\n",
    "from typing import List, Tuple\n",
    "from distutils.util import strtobool\n",
    "\n",
    "# define boolean to check if the notebook is run for the purposes of autograding\n",
    "AUTOGRADING = strtobool(os.environ.get(\"AUTOGRADING\", \"false\"))\n",
    "\n",
    "\n",
    "# seed the random number generators\n",
    "# https://pytorch.org/docs/stable/notes/randomness.html\n",
    "def manual_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Set manual seeds\n",
    "\n",
    "    Args:\n",
    "        seed: Random seed to sett.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "class DirectoryNotFoundException(Exception):\n",
    "    \"\"\"\n",
    "    Exception raised when a specified directory is not found.\n",
    "\n",
    "    This exception is used to signal that an operation which expects\n",
    "    a certain directory to exist has failed because the directory\n",
    "    was not found in the expected location. It extends the standard\n",
    "    Exception class and can be used to provide more specific error\n",
    "    handling that is distinct from general exceptions.\n",
    "\n",
    "    Attributes:\n",
    "        None - Inherits all attributes and methods from the base Exception class.\n",
    "\n",
    "    Methods:\n",
    "        None - Inherits all methods from the base Exception class and does not\n",
    "        define any additional behavior.\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class ModelTypeNotFoundException(Exception):\n",
    "    \"\"\"\n",
    "    Exception raised when a specified model type is not found.\n",
    "\n",
    "    This exception is used in contexts where a specific type of\n",
    "    model is expected to be present or accessible, but it cannot\n",
    "    be located or identified. The exception is particularly useful\n",
    "    in scenarios involving machine learning or data processing\n",
    "    pipelines where the absence of a required model type can lead\n",
    "    to failure of the process.\n",
    "\n",
    "    Attributes:\n",
    "        None - Inherits all attributes and methods from the base Exception class.\n",
    "\n",
    "    Methods:\n",
    "        None - Inherits all methods from the base Exception class and does not\n",
    "        define any additional behavior.\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "# check the directory for datasets\n",
    "DATASET_DIR = Path(\"datasets\") / \"pendulum_dataset\"\n",
    "if not DATASET_DIR.exists():\n",
    "    raise DirectoryNotFoundException(\n",
    "        f\"The pendulum dataset doesn't exist yet. Please run all cells in the `task_1a_extract_dataset` notebook.\"\n",
    "    )\n",
    "\n",
    "# create the directory for well-trained neural network models\n",
    "STATEDICTS_DIR = Path(\"statedicts\")\n",
    "STATEDICTS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9598ae47-d4af-4e75-a3c4-4e39d08f487e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa73308a91534e8f8f090c9eb8f660e4",
     "grade": false,
     "grade_id": "cell-4311cd7e06af6fe9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Then, we would like to prepare the training and test sets for the CNNs. In this assignment, we use two ways to predict the state: direct and indirect. Therefore, we prepare the datasets with two formats of labels, called $theta$ and $trig$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fb729a2-3692-402d-9074-356418a100ca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3cbf58f92c8c2430fe402198e2544087",
     "grade": false,
     "grade_id": "cell-9a4d22c564a03885",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class CNNDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch dataset class for a CNN, designed to load images and corresponding labels.\n",
    "\n",
    "    This class extends the PyTorch Dataset class, making it suitable for use with\n",
    "    PyTorch data loaders and other utilities. It is designed to load image data\n",
    "    and their associated labels from a specified directory, performing optional\n",
    "    transformations on the images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_path: PathLike, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset object with the data path, optional transform, and dataset length.\n",
    "\n",
    "        Args:\n",
    "            dataset_path: Path to the directory where image and label files are stored.\n",
    "            transform: A function/transform that takes in an image and returns a transformed version.\n",
    "        \"\"\"\n",
    "\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "        self.transform = transform\n",
    "\n",
    "        # determine the total dataset length\n",
    "        i = 0\n",
    "        while (self.dataset_path / f\"image{i}.npz\").exists():\n",
    "            i += 1\n",
    "        self.len = i\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Retrieves an image and its corresponding label at the given index `idx`. The image is\n",
    "        loaded from a .npz file, optionally transformed, and then returned along with its label\n",
    "        (both in position form and as sine and cosine components).\n",
    "\n",
    "        Args:\n",
    "            idx: The index of the image and label to be loaded.\n",
    "        Returns:\n",
    "            x: The image as a torch.Tensor, possibly transformed.\n",
    "            label_theta: The position label as a torch.FloatTensor\n",
    "            label_trig: A torch.FloatTensor with two elements, representing the sine and cosine\n",
    "               components of the label.\n",
    "        \"\"\"\n",
    "        x = np.array(np.load(self.dataset_path / f\"image{idx}.npz\")[\"arr_0\"])\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        theta = np.load(self.dataset_path / f\"label{idx}.npz\")[\"arr_0\"]\n",
    "        sin = np.sin(theta)\n",
    "        cos = np.cos(theta)\n",
    "\n",
    "        theta = torch.tensor(np.array(theta)).type(torch.FloatTensor)\n",
    "        trig = torch.tensor(np.array([sin, cos])).type(torch.FloatTensor)\n",
    "        return x, theta, trig\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the total number of samples available in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            self.len: The total number of images (or samples) in the dataset.\n",
    "        \"\"\"\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b46702-f5db-49c1-b867-283c163c99f5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a282dcdc7c4cfff2e4adc043895c973",
     "grade": false,
     "grade_id": "cell-2795060e8922d903",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The size of the original image data is `500x500x3`, and we want to compress it into a smaller size to save the computation time. Therefore, we add _transforms_ for preprocessing data when building our `Dataset` object. We first use the `ToTensor()` method in PyTorch to convert pixel data distributed between `[0, 255]` into floating point tensor data distributed between `[0, 1]`. Then, we use `Grayscale()` to convert the RGB images into grayscale images. _**Optional question**: Why can we convert the input color image into a grayscale image in this task? Can this be done for all tasks?_ Then we use `CenterCrop()` to extract the center `240x240`px area of the original image, and use `Resize()` to downsample the image to `24x24`px. With this preprocessing pipeline, we obtained training data with each input sample being a tensor of dimension `24x24x1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79320e4f-fa96-471a-a0fb-530542fffc5d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69b14674db259b6dd1ad944cee27e15d",
     "grade": false,
     "grade_id": "cell-f72b9c17c60bb8b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "IMAGE_TRANSFORM = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Grayscale(),\n",
    "        transforms.CenterCrop(240),\n",
    "        transforms.Resize((24, 24)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97ca7d0-d7bd-496a-a99c-4530c04b6e85",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "136aa5f8849fc62f325c486f5e1db1e4",
     "grade": false,
     "grade_id": "cell-4dc1782a7fb39615",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It is worth noting that, this `DataLoader` returns three variables: the image $x$, the corresponding label $\\theta$, and $[\\sin \\theta, \\cos \\theta]$ data. The `DataLoader` is consistent for direct and indirect prediction tasks. You must be careful about using underscores in your code to ignore unnecessary return values for the task.\n",
    "\n",
    "In this task, we divide the entire data set into three parts: training set, validation set, and test set. The split ratio between training, validation, and testing is 50%, 20%, and 30%. By calling the `load_dataloaders` method, the data loaders for training, validation, and testing are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffb5d854-74c7-4c8b-9bbf-e6a609165a27",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bff80a2e0217ae53d43b9475f1f2f0ff",
     "grade": false,
     "grade_id": "cell-a08f7e2a8ff6aed9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 3600 samples\n"
     ]
    }
   ],
   "source": [
    "def load_dataloaders(\n",
    "    dataset_path: PathLike,\n",
    "    val_ratio: float = 0.2,\n",
    "    test_ratio: float = 0.3,\n",
    "    batch_size: int = 32,\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Loads and returns data loaders for training, validation, and testing datasets.\n",
    "\n",
    "    Args:\n",
    "        dataset_path: Path to the directory where the dataset is stored.\n",
    "        val_ratio: The ratio of the dataset to be used for validation.\n",
    "                    It must be between 0 and 1. Defaults to 0.2.\n",
    "        test_ratio: The ratio of the dataset to be used for testing.\n",
    "                    It must be between 0 and 1. Defaults to 0.3.\n",
    "        batch_size: The number of samples per batch to load. Defaults to 32.\n",
    "\n",
    "    Returns:\n",
    "        train_dataloader: The DataLoader instances for the training.\n",
    "        val_dataloader: The DataLoader instances for the validation.\n",
    "        test_dataloader: The DataLoader instances for the testing.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the sum of val_ratio and test_ratio is larger than 1.0\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        0.0 <= val_ratio <= 1.0\n",
    "    ), \"Validation ratio needs to be in the interval [0, 1].\"\n",
    "    assert 0.0 <= test_ratio <= 1.0, \"Test ratio needs to be in the interval [0, 1].\"\n",
    "    assert (\n",
    "        val_ratio + test_ratio\n",
    "    ) <= 1.0, \"The sum of val and test ratio needs to be in the interval [0, 1].\"\n",
    "\n",
    "    dataset = CNNDataset(dataset_path, transform=IMAGE_TRANSFORM)\n",
    "    print(f\"The dataset contains {len(dataset)} samples\")\n",
    "\n",
    "    train_size = int(len(dataset) * (1.0 - val_ratio - test_ratio))\n",
    "    val_size = int(len(dataset) * val_ratio)\n",
    "    test_size = int(len(dataset) * test_ratio)\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        dataset, [train_size, val_size, test_size]\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = load_dataloaders(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb41372-69cb-4371-b8a8-fd01bcdb88db",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "59abd5959bf99e47189c2a1466261533",
     "grade": false,
     "grade_id": "cell-ec8e147966501c31",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We show the data after preprocessing as below. Do you still remember the colorful, high-resolution dataset samples in task 1a? Now, they have become a low-resolution grayscale image. Please feel free to visualize multiple samples by changing the respective `sample_idx` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29b268fa-f91b-4320-a7e3-e196a2057d69",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a6c11086460c074ca3937da6a0f95e6",
     "grade": false,
     "grade_id": "cell-0ec17e0cef5ed98a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_dataset_sample(dataset_path: str, sample_idx: int = 1200) -> None:\n",
    "    \"\"\"\n",
    "    Displays a visual representation of an example image from the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_path: Path to the directory where the dataset is stored.\n",
    "        sample_idx: The index of the sample to be visualized. Defaults to 1200.\n",
    "\n",
    "    Returns:\n",
    "        None: This function only displays an image and does not return any value.\n",
    "    \"\"\"\n",
    "    dataset = CNNDataset(dataset_path, transform=IMAGE_TRANSFORM)\n",
    "    img_example, _, _ = dataset[sample_idx]\n",
    "    img_example = img_example.permute(1, 2, 0)\n",
    "    plt.imshow(img_example, cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_dataset_sample(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8408af6-f43a-4c87-9c39-81ce8cd7451b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d8a2674cb2d7a747f0b624d65082b81",
     "grade": false,
     "grade_id": "cell-bb32bd4a5f16ab74",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We already have the function to build training, validation, and testing dataloaders. We will package the evaluation function in the following code block.\n",
    "\n",
    "Here, during the process of training, we adopt Mean Square Error (MSE) as the loss by invoking `nn.MSELoss()` from PyTorch. During the evalutation process (also in `evaluate_model` method), we adopt Mean Absolute Error (MAE) as the error by invoking `nn.L1Loss()`. _**Optional question:** why do we use MSE in the training process instead of MAE?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac377ae6-7d7d-4af7-bc20-72d4168da9a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a90f6f201ea16a81bc323d3e3303873a",
     "grade": false,
     "grade_id": "cell-393b7b53419f6731",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "error_fn = nn.L1Loss(reduction=\"mean\")\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model: nn.Module, eval_loader: DataLoader, model_type: str = \"theta\"\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a theta model on a test dataset.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model to be evaluated.\n",
    "        eval_loader: The DataLoader for the evaluation dataset.\n",
    "        model_type: The type of model to be evaluated ('theta' or 'trig').\n",
    "                                    Defaults to 'theta'.\n",
    "\n",
    "    Returns:\n",
    "        loss: The mean loss of the model on the valuation dataset.\n",
    "        error: The mean error of the model on the evaluation dataset.\n",
    "\n",
    "    Raises:\n",
    "        ModelTypeNotFoundException: If the provided model_type is neither 'theta' nor 'trig'.\n",
    "\n",
    "    \"\"\"\n",
    "    running_loss = 0.0\n",
    "    running_error = 0.0\n",
    "    count = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (x, theta, trig) in enumerate(eval_loader):\n",
    "            if model_type == \"theta\":\n",
    "                theta_hat = model(x).squeeze(1)\n",
    "\n",
    "                batch_loss = loss_fn(theta, theta_hat)\n",
    "                batch_error = error_fn(theta, theta_hat)\n",
    "            elif model_type == \"trig\":\n",
    "                trig_hat = model(x)\n",
    "                sin_hat = trig_hat[:, 0]\n",
    "                cos_hat = trig_hat[:, 1]\n",
    "                sin = trig[:, 0]\n",
    "                cos = trig[:, 1]\n",
    "                angle_hat = torch.atan2(sin_hat, cos_hat)\n",
    "                angle = torch.atan2(sin, cos)\n",
    "\n",
    "                batch_loss = loss_fn(trig, trig_hat)\n",
    "                batch_error = error_fn(angle, angle_hat)\n",
    "            else:\n",
    "                raise ModelTypeNotFoundException(\n",
    "                    f\"The model type should be either theta or trig.\"\n",
    "                )\n",
    "\n",
    "            running_loss += batch_loss.item()\n",
    "            running_error += batch_error.item()\n",
    "            count += 1\n",
    "\n",
    "    loss = running_loss / count\n",
    "    error = running_error / count\n",
    "\n",
    "    return loss, error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4852e8-f300-4c47-8d83-1151f6eb2fb4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "00caecaf1d2ebf545e0444e4132b8b30",
     "grade": false,
     "grade_id": "cell-508be3dc7526b037",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1b.1 - Learn to predict angles directly (9.5 points)\n",
    "We are going to create models that try to predict the link angles $\\hat{\\theta}$ given an image of the robot, so $\\theta \\approx \\hat{\\theta} =M(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8021a8-dc73-4bfe-ac92-f59089ee764b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f66b107dc5c647034896422eb4adee78",
     "grade": false,
     "grade_id": "cell-9161da53b6982892",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Tasb 1b.1.1 - Create the CNN model (3.5 points)\n",
    "It's time to create a CNN model. Here, create a PyTorch model class called `CNNTheta()`.\n",
    "* Start with a convolutional layer `torch.nn.Conv2d(...)` with 32 as the output channel number and a kernel size of `3x3`. This convolutional layer should be followed by a ReLU activation function.\n",
    "* Then add an average pooling layer `torch.nn.AvgPool2d(...)` with a pooling kernel size of `2x2`.\n",
    "* Add another convolutional layer with `10` as the output channel number and `3x3` as the kernel size. This convolutional layer is also followed by a ReLU function.\n",
    "* Add another average pooling layer with the kernel size of `2x2`.\n",
    "* Flatten the output of the pooling layer using `torch.nn.Flatten()`.\n",
    "* Add a fully connected layer with `30` as the output channel number, with a ReLU activation function.\n",
    "* Finally, add another fully connected layer without activation function. Remember, the number of output units must match the dimension of the target data, which is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ea65aa8-b7e8-44f2-84f5-740ed8d04e34",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e2b51a861463d4cf7fb45f78c43fe72",
     "grade": true,
     "grade_id": "cell-673fceb9cff47145",
     "locked": false,
     "points": 3.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable model parameters:  8071\n"
     ]
    }
   ],
   "source": [
    "\"\"\"TASK 1b.1.1: CREATE THETA MODEL HERE\"\"\"\n",
    "class CNNTheta(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNTheta, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        self.avg1 = nn.AvgPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=10, kernel_size=3)\n",
    "        self.avg2 = nn.AvgPool2d(kernel_size=2)\n",
    "        self.flat = torch.nn.Flatten()\n",
    "        self.fc1 = nn.Linear(in_features=160, out_features=30)\n",
    "        self.fc2 = nn.Linear(in_features=30, out_features=1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.avg1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.avg2(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "#raise NotImplementedError()\n",
    "\"\"\"TASK 1b.1.1: END\"\"\"\n",
    "\n",
    "model_theta = CNNTheta()\n",
    "total_params_theta = sum(p.numel() for p in model_theta.parameters())\n",
    "print(\"Total number of trainable model parameters: \", total_params_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08f951f-a253-4a21-a618-0ed4fe97d699",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b65910e9ca684feffc50896218b0a67",
     "grade": false,
     "grade_id": "cell-d0d1072d05ddf82c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Tasb 1b.1.2 - Train the model (4 points)\n",
    "Now that the model is defined, we can train it using the training dataset. Train the model for 50 epochs with the training data `train_loader`. Tune the learning rate in the optimizer to give the best performance on the validation and test set over the 50 epochs. For each training loop, it would be useful to test the model performance on the validation set to see if it is overfitting the training data or can generalize well. Do this 3 times and use the validation dataset `val_loader` with the function `evaluate_model` to get the validation result. \n",
    "\n",
    "**Hints:** \n",
    "- When we iterate over the training set, we use `(x, theta, _)` or something similar where `_` is used to ignore `trig`.\n",
    "- Reduce the number of epochs and lower the number of runs to `1` while getting your model working and setting the optimal learning rate. \n",
    "- Try varying the learning rate in the `1e-1` to `1e-5` range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "13327da5-4d1a-4e0c-a730-e80f87893907",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67438125ab90285988e5340e796c4b0f",
     "grade": false,
     "grade_id": "cell-18afa18aa1659afa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# set the number of runs (i.e., different random seeds)\n",
    "num_runs = 3  # Change to 1 until you get it to work once\n",
    "# number of epochs we train each model for\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1c98235-5396-4618-bf07-8264c2fe29ab",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9688eb204787ca1646e7c8a75004ab7b",
     "grade": true,
     "grade_id": "cell-0faed96316d9f061",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run 0\n",
      "Run 0 finished with a test loss of 0.1521 rad^2 and a mean test error of 0.1348 rad.\n",
      "Starting run 1\n",
      "Run 1 finished with a test loss of 0.1496 rad^2 and a mean test error of 0.1381 rad.\n",
      "Starting run 2\n",
      "Run 2 finished with a test loss of 0.2005 rad^2 and a mean test error of 0.2661 rad.\n"
     ]
    }
   ],
   "source": [
    "if not AUTOGRADING:\n",
    "    \"\"\"TASK 1b.1.2: TRAIN MODEL HERE\"\"\"\n",
    "    for run in range(num_runs):\n",
    "        print(f\"Starting run {run}\")\n",
    "        # set new random seed\n",
    "        manual_seed(seed=run)\n",
    "        # this code reinitializes the parameters of the model on each loop\n",
    "        model_theta = CNNTheta()\n",
    "        loss_fn = nn.MSELoss()\n",
    "        # define the optimizer\n",
    "        optimizer = torch.optim.SGD(model_theta.parameters(), lr=5e-3)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for x, theta,_ in train_loader:\n",
    "                theta_hat = model_theta(x).squeeze()\n",
    "                loss = loss_fn(theta_hat, theta)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            for x_val,theta_val,_ in val_loader:\n",
    "                pred_val = model_theta(x_val).squeeze()\n",
    "                loss_val = loss_fn(pred_val, theta_val)\n",
    "            \n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        run_test_loss, run_test_error = evaluate_model(\n",
    "            model_theta, test_loader, model_type=\"theta\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Run {run} finished with a test loss of {run_test_loss:.4} rad^2 and a mean test error of {run_test_error:.4} rad.\"\n",
    "        )\n",
    "\n",
    "        # save the model\n",
    "        torch.save(\n",
    "            model_theta.state_dict(),\n",
    "            STATEDICTS_DIR / f\"task_1b_theta_model_run-{run}.pth\",\n",
    "            _use_new_zipfile_serialization=False,\n",
    "        )\n",
    "    \"\"\"TASK 1.1.2: END\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359bbf11-d398-45c6-8530-41d350d366d0",
   "metadata": {},
   "source": [
    "Evaluate the final result on the `test_loader` using the function `evaluate_model`. The `model_type` is `theta` in this situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "61f99b36-8fd7-4edf-90f7-de6f6362ab69",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb4a053eef9acc1b7248aa698783c754",
     "grade": false,
     "grade_id": "cell-734fc06bea8d4bdf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error of theta model across runs: 0.1797 += 0.06113 rad.\n"
     ]
    }
   ],
   "source": [
    "# evaluate the theta model on the test set\n",
    "test_error_across_runs_theta = np.zeros((num_runs,))\n",
    "for run in range(num_runs):\n",
    "    model_theta.load_state_dict(\n",
    "        torch.load(STATEDICTS_DIR / f\"task_1b_theta_model_run-{run}.pth\")\n",
    "    )\n",
    "    run_test_loss, run_test_error = evaluate_model(\n",
    "        model_theta, test_loader, model_type=\"theta\"\n",
    "    )\n",
    "    test_error_across_runs_theta[run] = run_test_error\n",
    "print(\n",
    "    f\"Prediction error of theta model across runs: {np.mean(test_error_across_runs_theta):.4} += {np.std(test_error_across_runs_theta):.4} rad.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030a4085-8fd3-4d75-97d1-61dbfe9e3ade",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0abd9ba17a8a37e477636ce1f66aceca",
     "grade": false,
     "grade_id": "cell-9460d7f8f74cd5ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Tasb 1b.1.3 - Analyse model performance (2 points)\n",
    "Evaluate the trained model's accuracy by examining the average error in the link angle prediction. Please analyze the training loss curve: does the training loss decrease step by step? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a17da5d-7d20-4475-a19f-a0656816e940",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5a0a3d75d6d4c06040d71e43d4f0c50",
     "grade": true,
     "grade_id": "cell-7e66bf3c2ba23099",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53ea3a6-4f4c-4ca9-a2ee-edd4c6d9ed40",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a789cb1cd78b3dff414ca7c2ae04817",
     "grade": false,
     "grade_id": "cell-cf8f9f512c93eb31",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We always use a separate test dataset to evaluate a trained model. What is the reason for this? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f23ccb-37a6-4e30-af0e-96f8cc7b8fe1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09234ca54e2dcf141f3e3bbdcf266242",
     "grade": true,
     "grade_id": "cell-98110783648b05de",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ab22cb-baa1-4ca5-9df1-9196dd9cfca8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a8194c2696729b09d25284f62f78409",
     "grade": false,
     "grade_id": "cell-24f8c2fc51dd1023",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1b.2 - Indirectly predict the angle (10.5 points)\n",
    "We are going to improve the accuracy by pre-processing the target data. Specifically, we will create a model $M_{trig}$ that learns to predict $\\sin(\\theta)$ and $\\cos(\\theta)$ for the pendulum instead of directly predicting $\\theta$. Then, we can use the inverse tangent function to retrieve an estimate of $\\theta$ for both links.\n",
    "\n",
    "*Note:* In practice you would use the `atan2` implementation, as the regular arctangent only covers $[-\\frac{1}{2}\\pi, \\frac{1}{2}\\pi]$} $\\theta=\\arctan(\\frac{\\sin(\\theta)}{\\cos(\\theta)})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3065c32d-f32a-43c0-8909-8ff686ddcf1a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1508220f81a96c27c65b49a9a8d54f1c",
     "grade": false,
     "grade_id": "cell-fdb86715e0324415",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 1b.2.1 - Create the model (3.5 points)\n",
    "Copy the model you created in Task 1.1. Change the number of hidden units of the final layer from 1 to 2. We do so because we now want to predict two outputs ($\\sin(\\theta)$, $\\cos(\\theta)$) for each sample. The total number of the model parameters should increase slightly compared to Task 1.1. _**Optional question:** Why does the number of model parameters increase?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9354f229-7ded-47c8-8506-f90c60016919",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40ddaf9e869460c44e8ceaf6855155d5",
     "grade": true,
     "grade_id": "cell-0fa6b5c9dcd43530",
     "locked": false,
     "points": 3.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of model parameters:  8102\n"
     ]
    }
   ],
   "source": [
    "\"\"\"TASK 1b.2.1: CREATE TRIGONOMETRIC MODEL HERE\"\"\"\n",
    "\n",
    "class CNNTrig(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNTrig, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        self.avg1 = nn.AvgPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=10, kernel_size=3)\n",
    "        self.avg2 = nn.AvgPool2d(kernel_size=2)\n",
    "        self.flat = torch.nn.Flatten()\n",
    "        self.fc1 = nn.Linear(in_features=160, out_features=30)\n",
    "        self.fc2 = nn.Linear(in_features=30, out_features=2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.avg1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.avg2(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "\"\"\"TASK 1b.2.1: END\"\"\"\n",
    "\n",
    "model_trig = CNNTrig()\n",
    "total_params_trig = sum(p.numel() for p in model_trig.parameters())\n",
    "print(\"total number of model parameters: \", total_params_trig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e87a4f-d8d0-4134-881a-0a79aa54b85e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42ef35ba68bbb13a7149533dda929eea",
     "grade": false,
     "grade_id": "cell-f3a53b5a2f975228",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 1b.2.2 - Train the model (4 points)\n",
    "Now train a model with the same training hyperparameters as in task 1.1 but using the the trigonometric labels. When dealing with the return value of the `train_loader,` be careful to make sure that the output is the $[\\sin(\\theta)$, $\\cos(\\theta)]$ instead of $\\theta$. Tune the learning rate in the optimizer to give the best performance on the validation and test set over the 50 epochs. Do this 3 times like the previous task.\n",
    "\n",
    "**Hint:** \n",
    "- We utilize the third instead of the second item in the tuple returned by the dataloader. Therefore, when we iterate over the training set, we use `(x, _, trig)` where `_` is applied to ignore `theta`. Here, `trig` is a tensor containing $\\sin$ value and $\\cos$ value\n",
    "- Reduce the number of epochs and lower the number of runs to `1` while getting your model working. \n",
    "- Try in the range `1e-1` to `1e-5` for the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e23030e9-be70-4ea0-8621-1e10c687bf38",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea0a78c45020729191bbcf80bee6477c",
     "grade": false,
     "grade_id": "cell-f04acce798995efb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# set the number of runs (i.e., different random seeds)\n",
    "num_runs = 3  # Change to 1 until you get it to work once\n",
    "# number of epochs we train each model for\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bd1861b0-0ff7-456c-b8eb-d697ef26a8d5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce01085d02ea5a2b8e95d39d4e450b73",
     "grade": true,
     "grade_id": "cell-92d614bf44b02cde",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run 0\n",
      "Run 0 finished with a test loss of 0.0004255 and a mean test error of 0.02862 rad.\n",
      "Starting run 1\n",
      "Run 1 finished with a test loss of 0.0004052 and a mean test error of 0.01901 rad.\n",
      "Starting run 2\n",
      "Run 2 finished with a test loss of 0.0002419 and a mean test error of 0.01742 rad.\n"
     ]
    }
   ],
   "source": [
    "if not AUTOGRADING:\n",
    "    \"\"\"TASK 1b.2.2: TRAIN TRIGONOMETRIC MODEL HERE\"\"\"\n",
    "    for run in range(num_runs):\n",
    "        print(f\"Starting run {run}\")\n",
    "        # set new random seed\n",
    "        manual_seed(seed=run)\n",
    "        # this code reinitializes the parameters of the model on each loop\n",
    "        model_trig = CNNTrig()\n",
    "        loss_fn = nn.MSELoss()\n",
    "        # define the optimizer\n",
    "        optimizer = torch.optim.SGD(model_trig.parameters(), lr=1e-2)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for x,_,trig in train_loader:\n",
    "                trig_hat = model_trig(x).squeeze()\n",
    "                loss = loss_fn(trig_hat, trig)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            for x_val,_, trig_val in val_loader:\n",
    "                pred_val = model_trig(x_val).squeeze()\n",
    "                loss_val = loss_fn(pred_val, trig_val)\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        run_test_loss, run_test_error = evaluate_model(\n",
    "            model_trig, test_loader, model_type=\"trig\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Run {run} finished with a test loss of {run_test_loss:.4} and a mean test error of {run_test_error:.4} rad.\"\n",
    "        )\n",
    "\n",
    "        # save the model\n",
    "        torch.save(\n",
    "            model_trig.state_dict(),\n",
    "            STATEDICTS_DIR / f\"task_1b_trig_model_run-{run}.pth\",\n",
    "            _use_new_zipfile_serialization=False,\n",
    "        )\n",
    "    \"\"\"TASK 1b.2.2: END\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff2510",
   "metadata": {},
   "source": [
    "Evaluate the final result on the `test_loader` using the function `evaluate_model`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d960c0a6-b525-44d9-9886-97e100d7970b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f25b115cab9d168877c02874cf1852e",
     "grade": false,
     "grade_id": "cell-47ab7326c7589e4e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error of trig model across runs: 0.02168 += 0.004945 rad.\n"
     ]
    }
   ],
   "source": [
    "# evaluate the trig model on the test set\n",
    "test_error_across_runs_trig = np.zeros((num_runs,))\n",
    "for run in range(num_runs):\n",
    "    model_trig.load_state_dict(\n",
    "        torch.load(STATEDICTS_DIR / f\"task_1b_trig_model_run-{run}.pth\")\n",
    "    )\n",
    "    run_test_loss, run_test_error = evaluate_model(\n",
    "        model_trig, test_loader, model_type=\"trig\"\n",
    "    )\n",
    "    test_error_across_runs_trig[run] = run_test_error\n",
    "print(\n",
    "    f\"Prediction error of trig model across runs: {np.mean(test_error_across_runs_trig):.4} += {np.std(test_error_across_runs_trig):.4} rad.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df36a088-36e8-4f7b-982c-7b48c7bc5fd2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "01be8a369ef830c2061240a3243297fb",
     "grade": false,
     "grade_id": "cell-8c8760e8514e9fbc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 1b.2.3 - Analyse model performance (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3be0d3-ee31-421a-87b2-1e380f25d97a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbcff7117462baee761aab716eefec83",
     "grade": false,
     "grade_id": "cell-f3f9f302951f0706",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Analyse the model's training loss. Does your training loss decrease step by step? Do you find some magnitude difference between the training loss of $M_{trig}$ with the previous one of $M_θ$? Can you explain why they are different? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fcc579-4edf-4cda-8abe-7ee644349854",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a1808414d6d10b58a139a02791a5278",
     "grade": true,
     "grade_id": "cell-f9f43202ce5d05aa",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771d962f-3818-453e-9ac5-0b68672a9ae0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d07cb15513aedd98f1abf9dda736081f",
     "grade": false,
     "grade_id": "cell-7860d8b0c325435f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Compare the prediction estimates for $M_{trig}$ with the plot for $M_θ$. Why does indirectly predicting the angle improve the prediction accuracy? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eae4bd-667e-40b3-8bcb-b09b91c466d4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f46857304aceb30926d815826deb132",
     "grade": true,
     "grade_id": "cell-c3499d19e9853c15",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bebf84-5ed5-4305-a5e1-464272771680",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fb6797f63a583b32d26db30a0d628ede",
     "grade": false,
     "grade_id": "cell-075334b7096b05f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Why is it not sufficient to predict only sin(θ) and use its inverse θ = arcsin(sin(θ)) to get an estimate of the angle? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe63d97-7921-4534-8008-b105e3520ebf",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ac17047ececf4ebade6c286795823e4",
     "grade": true,
     "grade_id": "cell-7425567558dc530e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc52e93b-ed7c-4bd4-928b-7ab7b48ebea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
